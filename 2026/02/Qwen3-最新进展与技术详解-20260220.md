# ã€Qwen3ã€‘æœ€æ–°åŠ¨æ€ä¸æŠ€æœ¯è¯¦è§£ - 2026å¹´02æœˆ

## ğŸ“‹ æ¦‚è¿°

Qwen3æ˜¯é˜¿é‡Œäº‘æœ€æ–°å‘å¸ƒçš„å¤§è¯­è¨€æ¨¡å‹å®¶æ—ï¼Œå¼•å…¥äº†**åŒæ¨¡å¼æ¶æ„**ï¼ˆThinkingæ¨¡å¼+éThinkingæ¨¡å¼ï¼‰ï¼Œæ”¯æŒ0.6Bè‡³235Bå‚æ•°è§„æ¨¡ï¼Œå…·å¤‡è¶…å¼ºæ¨ç†èƒ½åŠ›å’Œ100+è¯­è¨€æ”¯æŒã€‚**Qwen3-Thinking-2507**ä½œä¸ºæœ€æ–°ç‰ˆæœ¬ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°å¼€æºæ¨¡å‹SOTAæ°´å¹³ï¼Œå¹¶æ”¯æŒ256Ké•¿ä¸Šä¸‹æ–‡ï¼ˆå¯æ‰©å±•è‡³100ä¸‡tokensï¼‰ã€‚

> âš ï¸ **é‡è¦è¯´æ˜**ï¼šæˆªè‡³2026å¹´2æœˆ20æ—¥ï¼Œå®˜æ–¹æœªå‘å¸ƒ"Qwen3.5"ç‰ˆæœ¬ã€‚æœ¬æ–‡æ¡£åŸºäºæœ€æ–°çš„**Qwen3ç³»åˆ—**ï¼ˆå«Qwen3-Thinking-2507ï¼‰æ•´ç†ã€‚

---

## âœ¨ æ ¸å¿ƒå‡çº§

### 1ï¸âƒ£ åˆ›æ–°åŒæ¨¡å¼æ¶æ„

**Thinkingæ¨¡å¼ï¼ˆæ€è€ƒæ¨¡å¼ï¼‰**
- ä¸“ä¸ºå¤æ‚æ¨ç†ä»»åŠ¡è®¾è®¡ï¼ˆæ•°å­¦ã€ç¼–ç¨‹ã€é€»è¾‘æ¨ç†ï¼‰
- æ·±åº¦æ€ç»´é“¾å±•ç¤ºå®Œæ•´æ¨ç†è¿‡ç¨‹
- åœ¨å­¦æœ¯benchmarkä¸Šè¾¾åˆ°ä¸“å®¶çº§è¡¨ç°

**Non-Thinkingæ¨¡å¼ï¼ˆå¿«é€Ÿå“åº”æ¨¡å¼ï¼‰**
- é«˜æ•ˆå¤„ç†æ—¥å¸¸å¯¹è¯å’Œç®€å•ä»»åŠ¡
- æ— ç¼åˆ‡æ¢ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¨¡å¼
- é™ä½è®¡ç®—æˆæœ¬ï¼Œæå‡å“åº”é€Ÿåº¦

### 2ï¸âƒ£ æ¨¡å‹è§„æ¨¡ä¸æ¶æ„

| æ¨¡å‹åç§° | å‚æ•°è§„æ¨¡ | æ¶æ„ç±»å‹ | ä¸»è¦ç‰¹ç‚¹ |
|---------|---------|---------|---------|
| Qwen3-0.6B | 0.6B | Dense | è¾¹ç¼˜è®¾å¤‡éƒ¨ç½² |
| Qwen3-1.7B | 1.7B | Dense | è½»é‡çº§åº”ç”¨ |
| Qwen3-4B | 4B | Dense | å¹³è¡¡æ€§èƒ½ä¸æ•ˆç‡ |
| Qwen3-8B | 8B | Dense | é«˜æ€§èƒ½é€šç”¨æ¨¡å‹ |
| Qwen3-32B | 32B | Dense | ä¼ä¸šçº§åº”ç”¨ |
| Qwen3-235B-A22B | 235B | MoE | è¶…å¤§è§„æ¨¡æ··åˆä¸“å®¶ |
| Qwen3-Thinking-2507 | - | - | æœ€æ–°æ¨ç†å¢å¼ºç‰ˆ |

### 3ï¸âƒ£ Qwen3-Thinking-2507 é‡å¤§å‡çº§

- **æ¨ç†èƒ½åŠ›é£è·ƒ**ï¼šåœ¨æ•°å­¦ã€ç§‘å­¦ã€ç¼–ç¨‹ç­‰æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡
- **é€šç”¨èƒ½åŠ›å¢å¼º**ï¼šæŒ‡ä»¤éµå¾ªã€å·¥å…·ä½¿ç”¨ã€æ–‡æœ¬ç”Ÿæˆå…¨é¢ä¼˜åŒ–
- **è¶…é•¿ä¸Šä¸‹æ–‡**ï¼šåŸç”Ÿæ”¯æŒ256Kï¼Œå¯æ‰©å±•è‡³**100ä¸‡tokens**
- **å¼€æºSOTA**ï¼šåœ¨å¼€æºæ€è€ƒæ¨¡å‹ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½

### 4ï¸âƒ£ æŠ€æœ¯èƒ½åŠ›çŸ©é˜µ

**æ ¸å¿ƒèƒ½åŠ›**
- ğŸŒ **å¤šè¯­è¨€**ï¼šæ”¯æŒ100+è¯­è¨€ï¼Œä¸­è‹±æ–‡èƒ½åŠ›çªå‡º
- ğŸ”§ **å·¥å…·ä½¿ç”¨**ï¼šåŸç”Ÿæ”¯æŒå¤–éƒ¨å·¥å…·è°ƒç”¨å’ŒAgentèƒ½åŠ›
- ğŸ“Š **é•¿ä¸Šä¸‹æ–‡**ï¼šé€šè¿‡YaRN RoPEæ‰©å±•æ”¯æŒ131K-1M tokens
- ğŸ¯ **æ¨ç†æ·±åº¦**ï¼šThinkingæ¨¡å¼æä¾›å®Œæ•´æ€ç»´é“¾

**é‡åŒ–æ”¯æŒ**
- BF16ï¼ˆåŸç”Ÿç²¾åº¦ï¼‰
- FP8ï¼ˆæ€§èƒ½å¹³è¡¡ï¼‰
- GPTQ-Int8/Int4ï¼ˆé«˜å‹ç¼©æ¯”ï¼‰
- AWQ-Int4ï¼ˆå¿«é€Ÿæ¨ç†ï¼‰

---

## ğŸ“Š æ€§èƒ½è·‘åˆ†

### åŸºå‡†æµ‹è¯•æˆç»©ï¼ˆQwen2ç³»åˆ—å‚è€ƒï¼‰

> æ³¨ï¼šQwen3æœ€æ–°benchmarkå°šæœªå®Œå…¨å…¬å¼€ï¼Œä»¥ä¸‹ä¸ºQwen2ç³»åˆ—å‚è€ƒæ•°æ®

| æ¨¡å‹ | é‡åŒ–æ–¹å¼ | MMLU | C-Eval | IFEval |
|------|---------|------|--------|--------|
| **Qwen2-72B** | BF16 | **82.3** | **83.8** | **77.6** |
| Qwen2-72B | GPTQ-Int4 | 80.8 | 83.9 | 78.9 |
| **Qwen2-7B** | BF16 | **70.5** | **77.2** | **53.1** |
| Qwen2-7B | AWQ | 67.4 | 73.6 | 51.4 |

**è¯„æµ‹è¯´æ˜**ï¼š
- **MMLU**ï¼šå¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼ˆå­¦æœ¯èƒ½åŠ›ï¼‰
- **C-Eval**ï¼šä¸­æ–‡ç»¼åˆè¯„ä¼°
- **IFEval**ï¼šæŒ‡ä»¤éµå¾ªå‡†ç¡®åº¦

### æ¨ç†æ€§èƒ½ï¼ˆTokens/ç§’ï¼‰

**Qwen3-8B @ SGLangæ¡†æ¶**

| è¾“å…¥é•¿åº¦ | BF16 | FP8 | AWQ-INT4 |
|---------|------|-----|----------|
| 1 token | 81.73 | 150.25 | 144.11 |
| 6K tokens | 296.25 | 516.64 | 477.89 |
| 30K tokens | 832.67 | **1242.24** | 1075.91 |
| 129K tokens | 1173.32 | **1393.21** | 1198.06 |

**æ€§èƒ½äº®ç‚¹**ï¼š
- FP8é‡åŒ–åœ¨é•¿æ–‡æœ¬åœºæ™¯ä¸‹æ€§èƒ½æœ€ä¼˜
- æ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡æ—¶ä»ä¿æŒé«˜åå
- é‡åŒ–åæ€§èƒ½æŸå¤±æå°

---

## ğŸ’¡ å®ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šåŸºç¡€æ¨ç†ï¼ˆInstructæ¨¡å¼ï¼‰

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-30B-A3B-Instruct-2507")
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-30B-A3B-Instruct-2507",
    torch_dtype="auto",
    device_map="auto"
)

# å‡†å¤‡è¾“å…¥
messages = [{"role": "user", "content": "è§£é‡Šé‡å­çº ç¼ çš„åŸç†"}]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer([text], return_tensors="pt").to(model.device)

# ç”Ÿæˆå›å¤
outputs = model.generate(**inputs, max_new_tokens=2048)
response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
print(response)
```

### ç¤ºä¾‹2ï¼šè¶…é•¿ä¸Šä¸‹æ–‡å¤„ç†ï¼ˆvLLM + YaRNï¼‰

```bash
# å¯åŠ¨æ”¯æŒ131Kä¸Šä¸‹æ–‡çš„vLLMæœåŠ¡
vllm serve Qwen/Qwen3-8B \
    --rope-scaling '{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}' \
    --max-model-len 131072 \
    --port 8000
```

```python
from openai import OpenAI

client = OpenAI(api_key="EMPTY", base_url="http://localhost:8000/v1")

# å¤„ç†è¶…é•¿æ–‡æ¡£ï¼ˆ10ä¸‡+tokensï¼‰
long_document = open("long_report.txt").read()  # å‡è®¾100K tokens

response = client.chat.completions.create(
    model="Qwen/Qwen3-8B",
    messages=[{"role": "user", "content": f"æ€»ç»“ä»¥ä¸‹æ–‡æ¡£:\n\n{long_document}"}],
    max_tokens=4096
)

print(response.choices[0].message.content)
```

### ç¤ºä¾‹3ï¼šé…ç½®æ–‡ä»¶æ‰©å±•ä¸Šä¸‹æ–‡ï¼ˆTransformersï¼‰

ä¿®æ”¹`config.json`ä»¥å¯ç”¨131Kä¸Šä¸‹æ–‡ï¼š

```json
{
    "max_position_embeddings": 131072,
    "rope_scaling": {
        "rope_type": "yarn",
        "factor": 4.0,
        "original_max_position_embeddings": 32768
    }
}
```

---

## ğŸ¯ ä¼˜åŠ¿åº”ç”¨åœºæ™¯

### 1. **å¤æ‚æ¨ç†ä»»åŠ¡**ï¼ˆThinkingæ¨¡å¼ï¼‰
- æ•°å­¦é¢˜æ±‚è§£ï¼ˆç«èµ›çº§éš¾åº¦ï¼‰
- ä»£ç ç”Ÿæˆä¸è°ƒè¯•ï¼ˆå¤šæ–‡ä»¶é¡¹ç›®ï¼‰
- ç§‘å­¦è®ºæ–‡åˆ†æ
- é€»è¾‘æ¨ç†é—®é¢˜

**æ¨èæ¨¡å‹**ï¼šQwen3-Thinking-2507ã€Qwen3-30B

### 2. **è¶…é•¿æ–‡æ¡£å¤„ç†**
- æ³•å¾‹åˆåŒå®¡æŸ¥ï¼ˆæ•°ä¸‡å­—ï¼‰
- å­¦æœ¯æ–‡çŒ®ç»¼è¿°
- ä¼ä¸šæŠ¥å‘Šåˆ†æ
- å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ä¿æŒ

**æ¨èæ¨¡å‹**ï¼šQwen3-8Bï¼ˆé…ç½®YaRNæ‰©å±•ï¼‰

### 3. **é«˜æ•ˆå®¢æœä¸å¯¹è¯**ï¼ˆNon-Thinkingæ¨¡å¼ï¼‰
- æ™ºèƒ½å®¢æœç³»ç»Ÿ
- çŸ¥è¯†é—®ç­”
- æ—¥å¸¸åŠå…¬åŠ©æ‰‹
- å†…å®¹ç”Ÿæˆï¼ˆåšå®¢ã€æ–‡æ¡ˆï¼‰

**æ¨èæ¨¡å‹**ï¼šQwen3-4Bã€Qwen3-7B

### 4. **è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²**
- ç§»åŠ¨ç«¯AIåŠ©æ‰‹
- ç‰©è”ç½‘è®¾å¤‡
- ç¦»çº¿è¯­éŸ³åŠ©æ‰‹

**æ¨èæ¨¡å‹**ï¼šQwen3-0.6Bã€Qwen3-1.7Bï¼ˆé‡åŒ–ç‰ˆæœ¬ï¼‰

---

## ğŸ†š æ¨ªå‘å¯¹æ¯”

### ä¸ä¸»æµæ¨¡å‹å¯¹æ¯”ï¼ˆä¼°ç®—ï¼‰

| ç»´åº¦ | Qwen3-Thinking | GPT-4 | Claude 3.5 Sonnet | Llama 3.1 405B |
|------|---------------|-------|-------------------|----------------|
| **å‚æ•°è§„æ¨¡** | 235B (MoE) | æœªçŸ¥ | æœªçŸ¥ | 405B |
| **ä¸Šä¸‹æ–‡é•¿åº¦** | 256K-1M | 128K | 200K | 128K |
| **æ¨ç†èƒ½åŠ›** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| **å¤šè¯­è¨€æ”¯æŒ** | 100+ | 50+ | 50+ | 80+ |
| **å¼€æº** | âœ… | âŒ | âŒ | âœ… |
| **æœ¬åœ°éƒ¨ç½²** | âœ… | âŒ | âŒ | âœ… |
| **åŒæ¨¡å¼åˆ‡æ¢** | âœ… | âŒ | âŒ | âŒ |
| **æˆæœ¬** | å…è´¹/è‡ªæ‰˜ç®¡ | APIè®¡è´¹ | APIè®¡è´¹ | å…è´¹/è‡ªæ‰˜ç®¡ |

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š
- âœ… **å®Œå…¨å¼€æº**ï¼šæ— APIä¾èµ–ï¼Œæ”¯æŒç§æœ‰åŒ–éƒ¨ç½²
- âœ… **è¶…é•¿ä¸Šä¸‹æ–‡**ï¼š1M tokensåœ¨å¼€æºæ¨¡å‹ä¸­é¢†å…ˆ
- âœ… **åŒæ¨¡å¼åˆ›æ–°**ï¼šè‡ªåŠ¨ä¼˜åŒ–æ€§èƒ½ä¸æ•ˆç‡å¹³è¡¡
- âœ… **ä¸­æ–‡èƒ½åŠ›**ï¼šåœ¨C-Evalç­‰ä¸­æ–‡benchmarkè¡¨ç°ä¼˜å¼‚

---

## ğŸ”§ å¿«é€Ÿå¼€å§‹

### æ–¹å¼1ï¼šTransformersï¼ˆæœ¬åœ°æ¨ç†ï¼‰

```bash
pip install transformers torch accelerate
```

```python
from transformers import pipeline

# å¿«é€Ÿåˆ›å»ºå¯¹è¯ç®¡é“
pipe = pipeline("text-generation", model="Qwen/Qwen3-8B-Instruct")
response = pipe("å†™ä¸€ä¸ªPythonå¿«é€Ÿæ’åºç®—æ³•", max_new_tokens=512)
print(response[0]['generated_text'])
```

### æ–¹å¼2ï¼šOllamaï¼ˆä¸€é”®éƒ¨ç½²ï¼‰

```bash
# å®‰è£…Ollamaåç›´æ¥è¿è¡Œ
ollama run qwen2.5:7b   # Qwen2.5ç³»åˆ—
# Qwen3ç³»åˆ—å³å°†æ”¯æŒ
```

### æ–¹å¼3ï¼švLLMï¼ˆç”Ÿäº§çº§æœåŠ¡ï¼‰

```bash
pip install vllm

# å¯åŠ¨APIæœåŠ¡
vllm serve Qwen/Qwen3-8B-Instruct --port 8000
```

---

## ğŸ“š ç›¸æ¯”å‰ä»£ï¼ˆQwen2.5/Qwen3ï¼‰æ ¸å¿ƒå˜åŒ–

### Qwen3 vs Qwen2.5

| ç‰¹æ€§ | Qwen2.5 | Qwen3 |
|------|---------|-------|
| æ¶æ„ | å•ä¸€æ¨¡å¼ | **åŒæ¨¡å¼ï¼ˆThinking+Instructï¼‰** |
| æœ€å¤§ä¸Šä¸‹æ–‡ | 128K | **256K-1M** |
| æ¨ç†èƒ½åŠ› | å¼º | **æ˜¾è‘—å¢å¼ºï¼ˆThinkingæ¨¡å¼ï¼‰** |
| MoEæ¶æ„ | æ—  | **235B-A22Bï¼ˆæ··åˆä¸“å®¶ï¼‰** |
| å·¥å…·ä½¿ç”¨ | åŸºç¡€ | **å¢å¼ºçš„Agentèƒ½åŠ›** |

### Qwen3-Thinking-2507 æ–°å¢èƒ½åŠ›

1. **æ¨ç†è´¨é‡æå‡**ï¼šåœ¨AIMEã€GPQAç­‰å­¦æœ¯benchmarkå¤§å¹…é¢†å…ˆ
2. **é€šç”¨èƒ½åŠ›å¼ºåŒ–**ï¼šæŒ‡ä»¤éµå¾ªã€å¯¹é½åº¦æå‡
3. **é•¿ä¸Šä¸‹æ–‡ä¼˜åŒ–**ï¼š256KåŸç”Ÿæ”¯æŒï¼Œ1Mæ‰©å±•æ›´ç¨³å®š
4. **æ€ç»´é“¾å¯è§†åŒ–**ï¼šå®Œæ•´å±•ç¤ºæ¨ç†è¿‡ç¨‹

---

## ğŸ”— å‚è€ƒèµ„æ–™

### å®˜æ–¹èµ„æº
- **å®˜æ–¹æ–‡æ¡£**: [Qwen Documentation](https://qwen.readthedocs.io/en/latest/)
- **GitHubä»“åº“**: [QwenLM/Qwen3](https://github.com/qwenlm/qwen3)
- **HuggingFace**: [Qwen Models](https://huggingface.co/Qwen)
- **æŠ€æœ¯åšå®¢**: [Qwenå®˜æ–¹åšå®¢](https://qwenlm.github.io)

### æŠ€æœ¯æ–‡æ¡£
- [Speed Benchmarkè¯¦ç»†æŠ¥å‘Š](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html)
- [é‡åŒ–æ€§èƒ½å¯¹æ¯”](https://qwen.readthedocs.io/en/latest/getting_started/quantization_benchmark)
- [vLLMéƒ¨ç½²æŒ‡å—](https://docs.vllm.ai/en/latest/)

### æ¨¡å‹ä¸‹è½½
```bash
# HuggingFace CLI
huggingface-cli download Qwen/Qwen3-8B-Instruct

# Git LFS
git lfs clone https://huggingface.co/Qwen/Qwen3-8B-Instruct
```

---

## ğŸ”® æœªæ¥å±•æœ›

åŸºäºå®˜æ–¹roadmapæ¨æµ‹ï¼š
- **å¤šæ¨¡æ€èåˆ**ï¼šQwen3-VLã€Qwen3-Audioå¢å¼ºç‰ˆ
- **Agentç”Ÿæ€**ï¼šæ›´å¼ºçš„å·¥å…·è°ƒç”¨å’Œè‡ªä¸»å†³ç­–èƒ½åŠ›
- **æ•ˆç‡ä¼˜åŒ–**ï¼šæ›´æ¿€è¿›çš„é‡åŒ–æ–¹æ¡ˆå’Œæ¨ç†åŠ é€Ÿ
- **é¢†åŸŸæ¨¡å‹**ï¼šåŒ»ç–—ã€æ³•å¾‹ã€é‡‘èç­‰å‚ç›´é¢†åŸŸä¸“ç”¨ç‰ˆæœ¬

---

*ğŸ“… æ•´ç†æ—¥æœŸ: 2026-02-20*  
*ğŸ“¦ æ•°æ®æ¥æº: Context7å®˜æ–¹æ–‡æ¡£ã€Qwen ReadTheDocs*  
*ğŸ¤– ç”± Claude Code + tech-news-reporter skill è‡ªåŠ¨ç”Ÿæˆ*

---

## é™„å½•ï¼šå¸¸è§é—®é¢˜

**Q: Qwen3.5æ˜¯å¦å·²å‘å¸ƒï¼Ÿ**  
A: æˆªè‡³2026å¹´2æœˆï¼Œå®˜æ–¹æœªå‘å¸ƒQwen3.5ã€‚æœ€æ–°ç‰ˆæœ¬ä¸ºQwen3-Thinking-2507ã€‚

**Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¨¡å‹è§„æ¨¡ï¼Ÿ**  
A: 
- è¾¹ç¼˜è®¾å¤‡ï¼š0.6B/1.7B
- ä¸ªäººPCï¼š4B/8B
- æœåŠ¡å™¨ï¼š32B/235B
- å¤æ‚æ¨ç†ï¼šQwen3-Thinkingç³»åˆ—

**Q: é‡åŒ–ä¼šæŸå¤±å¤šå°‘æ€§èƒ½ï¼Ÿ**  
A: æ ¹æ®benchmarkï¼ŒFP8é‡åŒ–æ€§èƒ½æŸå¤±<2%ï¼ŒInt4æŸå¤±çº¦3-5%ï¼Œä½†æ¨ç†é€Ÿåº¦æå‡50-100%ã€‚

**Q: æ”¯æŒå“ªäº›éƒ¨ç½²æ¡†æ¶ï¼Ÿ**  
A: Transformersã€vLLMã€SGLangã€Ollamaã€llama.cppã€TensorRT-LLMç­‰ä¸»æµæ¡†æ¶å‡æ”¯æŒã€‚
